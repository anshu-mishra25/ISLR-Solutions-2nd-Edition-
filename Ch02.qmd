# 2 Statistical Learning {.unnumbered}



## Conceptual


### Question 1

**(a) Better:** flexible method incorporates the data better when the sample size is large enough.

**(b) Worse:** The data set leads to the 'Curse of Dimensionality', flexible model will tend to be more overfitting, meaning it will try to follow the error (noise) too closely.

**(c) Better:** Flexible methods perform better on non-linear datasets as they have more degrees of freedom to approximate a non-linear.

**(d) Worse:** A flexible model would likely overfit, due to more closely fitting the noise in the error terms than an inflexible method. In other words, the data points will be far from *f* (ideal function to describe the data) if the variance of the error terms is very high. This hints that that *f* is linear and so a simpler model would be better to be able to estimate *f*.


### Question 2


**(a) Regression Problem:** Since it is a quantitative problem<br>
    **Inference:** We are interested in the factors which affect CEO salary<br>
    **n = 500**<br>
    **p = 3** (Profit, number of employees, industry)<br>

**(b) Classification Problem:** Since it is binary (success or failure)<br>
    **Prediction:** We are interested in the success or failure of the product<br>
    **n = 20**<br>
    **p = 13** (Price charged, marketing budget, competition price + 10 other)<br>

**(c) Regression Problem**<br>
    **Prediction:** We are interested in the % change in the USD/Euro exchange rate<br>
    **n = 52**<br>
    **p = 3** (% change in the US market, % change in the British market, % 
    change in the German market)<br>


### Question 3

**(a)**
![](images/Bias-variance-sketch.jpg "AI-generated"){width=70% height=70%}

**(b) Squared Bias:** Decreases with more flexibility (generally more flexible methods results in less bias)

**Variance:** Increases with more flexibility

**Training Error:** Continues to reduce as flexibility grows, meaning the model is too close to the training data.

**Test Error:** Decreases initially, and reaches the optimal point where it gets flat, then it starts to increase again, meaning an overfitted data

**Bayes (irreducible) Error:** Flat/Fixed, since it can't be reduced any more


### Question 4

**(a) Three real-life applications in which classification might be useful**

**1. Email Spam Detection**

* Response: Spam(1) or Not Spam(0)
* Predictors: some special words, special links, other
* Goal: Prediction --- we care more about classifying new emails correctly            than about interpreting which word exactly causes spam.

**2. Loan Default Risk**

* Respone: Default(1) or Won't Default(0)
* Predictors: income, credit score, education, etc
* Goal: Prediction/Inference --- banks want to predict default risk, but              also understand which factors matter most

**3. Stock Price**

* Response: Stock increases(1) or Stock goes down(0)
* Predictors: Financial Statements, Ratio Analysis, Competitors price, etc
* Goal: Prediction --- we are interested in making the future movements of            the stock price.

**(b) Three real-life applications in which regression might be useful:**

**1. House Price Estimation**

* Response: House price (in $).
* Predictors: square footage, number of bedrooms, location, age of house.
* Goal: Prediction — estimate the price of houses not yet sold.

**2. Salary Determination**

* Response: Employee salary (in $).
* Predictors: years of experience, education level, industry, job role.
* Goal: Inference — HR might want to know which factors (experience vs.                education) most strongly drive salaries.

**3. Insurance Premium Calculation**

* Response: Annual premium amount.
* Predictors: age, medical history, smoking status, BMI.
* Goal: Both — insurers need prediction for pricing policies, but also                 inference to understand key risk drivers.

**(c) Three real-life applications in which cluster analysis might be useful:**

**1. Customer Segmentation in Retail**

* Predictors: purchase history, shopping frequency, spending amounts.
* Goal: Identify customer groups (e.g., bargain-hunters vs. high spenders)
          for targeted marketing.

**2. Genetic Research**

* Predictors: gene expression levels.
* Goal: Cluster similar genes/patients to identify subtypes of diseases.

**3. Social Media Communities**

* Predictors: interaction patterns, hashtags used, follower networks.
* Goal: Find natural clusters of users (e.g., sports fans, political groups,           hobby communities).

### Question 5

**Advantages of a More Flexible Approach**

* Better fit to complex patterns: Can capture nonlinear relationships and       intricate interactions between variables.
* Lower bias: Since fewer assumptions are made about the functional form,       the model can adapt closely to the data.
* High predictive power: Often yields better test accuracy if enough data is     available.

**Disadvantages of a More Flexible Approach**

* Risk of overfitting: May capture noise in the training data instead of the     true signal.
* Higher variance: Predictions can change drastically with new data.
* Interpretability: Flexible models are often “black boxes” (e.g., random       forests, neural nets).
* Computational cost: More data and resources needed for training.

**Advantages of a Less Flexible Approach**

* Simplicity: Easy to implement, interpret, and explain to stakeholders.
* Low variance: Less sensitive to small changes in the dataset.
* Efficient: Works well with smaller datasets, requires less computation.
* Inference: Easier to test hypotheses and understand relationships between     predictors and response.

**Disadvantages of a Less Flexible Approach**

* High bias: May oversimplify reality by assuming linearity or ignoring         interactions.
* Poor fit for complex relationships: If the true relationship is nonlinear,    performance suffers.

**When to prefer More Flexible Approaches**

* When prediction accuracy is the main goal.
* When the relationship between predictors and response is complex and          nonlinear.
* When you have large amounts of data to reduce variance and prevent            overfitting.

**When to prefer Less Flexible Approaches**

* When the primary goal is inference/interpretability (understanding which      predictors matter).
* When you have limited data, flexible models overfit easily with small         datasets.
* When stakeholders (like regulators, executives) need transparent models.


### Question 6

**Parametric:** Assume a functional form (e.g., linear regression), then estimate a few parameters.

* Advantage: Works with small data, simple, interpretable, fast.
* Disadvantage: Risk of bias if model form is wrong, limited flexibility.

**Non-Parametric:** No strict form, model is shaped by the data (e.g., KNN, trees).

* Advantage: Very flexible, can capture complex patterns.
* Disadvantage: Needs lots of data, higher variance, less interpretable,        slower.


### Question 7

**(a) Computing the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0** 
```{r}
(data.set <- data.frame(
  "X1" = c(0,2,0,0,-1,1),
  "X2" = c(3,0,1,1,0,1),
  "X3" = c(0,0,3,2,1,1),
  "Y" = c("Red","Red","Red","Green","Green","Red")
  ))   # A given data set
```

```{r}
euclidian.distance <- 
  function(X, pred.data){
    # 'X' here represents a vector of points
    distance = sqrt((X[1]-pred.data[1])^2 +
                      (X[2]-pred.data[2])^2 +
                      (X[3]-pred.data[3])^2) 
    return(distance)
  }  # A function to compute euclidian distance
```

```{r}
X <- data.set[,-4]    # Taking only the given X co-ordinates
Prediction.coordinates <- c(0,0,0) # Prediction co-ordinates

distance <- numeric()
for(i in 1:nrow(X)){
  distance[i] = as.matrix(euclidian.distance(X[i,], Prediction.coordinates))
}

distance <- as.matrix(distance)

cat("\n",
  "dist(New,Obs1) =",distance[1,1],"\n",
  "dist(New,Obs2) =",distance[2,1],"\n",
  "dist(New,Obs3) =",distance[3,1],"\n",
  "dist(New,Obs4) =",distance[4,1],"\n",
  "dist(New,Obs5) =",distance[5,1],"\n",
  "dist(New,Obs6) =",distance[6,1],"\n"
  )

```

**(b)**

```{r}
(Y <- data.set[,4]) # All possible given ouputs

KNN <- 
  function(K){
    Y[which.min(abs(distance[,1]-K))]
  }

cat("K-Nearest Neighbour for K = 1 predicts it to be",KNN(1))
```
**(c)**
```{r}
cat("K-Nearest Neighbour for K = 3 predicts it to be",KNN(3))
```

## APPLIED


### Question 8

Running the 'ISLR2' package
```{r}
# install.packages("ISLR2")
library(ISLR2)
```

**(a)**
```{r}
college <- College # naming as per question
```

**(b)**
```{r}
# View(college) #
head(college)
```

**(c) **
```{r}
# i.
summary(college)
```

```{r}
# ii.
pairs(college[,1:10], cex = 0.1)
```


```{r}
# iii.
boxplot(Outstate ~ Private, data = college)
```


```{r}
# iv.
college$Elite <- as.factor(ifelse(college$Top10perc > 50, "Yes", "No"))

summary(college$Elite)
```


```{r}
# v.
par(mfrow = c(2,2))

# FOR COLLEGE APPLICATIONS RECEIVED
for(n in c(5,10,15,20)){
  hist(college$Apps, 
       xlab = "Number of Applications Received",
       main = "Histogram of College Application",
       breaks = n,
       xlim = c(0,20000))
}
```

```{r}
par(mfrow = c(2,2))

# FOR COLLEGE APPLICATIONS ACCEPTED
for(n in c(5,10,15,20)){
  hist(college$Accept, 
       xlab = "Number of Applications Accepted",
       main = "Histogram of College Application",
       breaks = n,
       xlim = c(0,20000))
}
```


```{r}
par(mfrow = c(1,1))
```

```{r}
# vi.
# Making a linear model with all variables
Model1 <- lm(Accept~.,data=College)
summary(Model1)

# Using this model to determine all the significant parameters which comes out 
# to be:
# -> Private
# -> Apps
# -> Enroll
# -> Top10perc
# -> Top25perc
# -> P.Undergrad
# -> Outstate
# -> perc.alumni
# -> Expend
# Making another model with these parameters only:

Model2 <- lm(Accept ~ Private + Apps + Enroll + Top10perc + Top25perc +
               P.Undergrad + Outstate + perc.alumni + Expend,
             data=College)
summary(Model2)
# Adjusted R-squared dropped but only by 0.0004, that doesn't make Model1 any
# better than Model2 with so much less parameters.

AIC(Model1)
AIC(Model2)
# AIC dropped

# Another Model with even less parameters
Model3 <- update(Model2, .~. -Private - P.Undergrad - perc.alumni)
summary(Model3)

AIC(Model3)
# AIC increased by ~2 points

anova(Model2, Model3)

# Another Model
Model4 <- update(Model2, .~. -Private)
summary(Model4)

AIC(Model2); AIC(Model4)

anova(Model2,Model4)
```

**Overall Model 2 is better, but if I wanted to use as less co-variates as possible in order to explain most of the response variable, I can use Model 4 as well.**


### Question 9

**(a)**
```{r}
head(Auto)
```

Quantitative: mpg, cylinders, displacement, horsepower, weight, acceleration,               year
Qualitative: origin, name

**(b)**
```{r}
apply(Auto[,1:7], 2, range)
```

**(c)**
```{r}
apply(Auto[,1:7], 2, mean)
apply(Auto[,1:7], 2, sd)
```

**(d)**
```{r}
Auto.reduced <- Auto[-c(10:85), ]

apply(Auto.reduced[,1:7], 2, range)
apply(Auto.reduced[,1:7], 2, mean)
apply(Auto.reduced[,1:7], 2, sd)
```

**(e)**
```{r}
pairs(Auto[,1:7], cex = 0.5, pch = 16)
cor(Auto[,1:7])
```
The covariates with high positive correlation are (more than 0.7):
cylinders vs displacement
cylinders vs horsepower
cylinders vs weight
displacement vs horsepower
displacement vs weight
horsepower vs weight

The covariates with high negative correlation are (less than -0.7):
mpg vs cylinders
mpg vs displacement
mpg vs horsepower
mpg vs weight

**(f)**

Yes, all the other variables except acceleration (maybe even year) are highly correlated and can be used to predict mpg.


### Question 10

**(a)**
```{r}
head(Boston)
```

```{r}
cat("\n",
    "Number of rows =",nrow(Boston),"\n",
    "Number of columns =",ncol(Boston))
```

Each 13 column suggests a variable affecting the price of the particular suburb.
There are 506 suburbs each with 13 explanatory variables.


**(b)**
```{r}
pairs(Boston, cex = 0.5)
cor(Boston)
```
* medv parameter has good positive correlation with rm and high negative        correlation with lstat

* indus parameter has high positive correlation with nox and tax, and high
  negative correlation with dis
    
* and many more...

**(c)**

crm (per capita crime rate by town) has the highest correlation with rad 
(index of accesssibility to radial highways) which is positive.

**(d)**

```{r}
# High Crime Rates
High.Crime.Rate <- 
  Boston$crim[Boston$crim > mean(Boston$crim) + 2*sd(Boston$crim)]
cat("There are",length(High.Crime.Rate),"suburbs which have high crime rate")
hist(High.Crime.Rate,
     xlab = "Crime Rate",
     main = "Histogram of Suburbs with High Crime Rates")
range(High.Crime.Rate)
```
```{r}
# High Tax Rates
High.Tax.Rate <- 
  Boston$tax[Boston$tax > mean(Boston$tax) + 2*sd(Boston$tax)]

cat("There are",length(High.Tax.Rate), "suburbs which have high tax rate")
```

```{r}
# High Pupil-teacher ratios
High.Pupil.teacher.ratios <- 
  Boston$ptratio[Boston$ptratio > mean(Boston$ptratio) + 2*sd(Boston$ptratio)]

cat("There are",length(High.Pupil.teacher.ratios),
    "suburbs which have high Pupil-teacher ratio")
```

**(e)**

cat("There are",sum(Boston$chas),"suburbs that bound the Charles river.")

**(f)**

cat("Median pupil-teacher ratio in town is",median(Boston$ptratio))

**(g)**
```{r}
which(Boston$medv == min(Boston$medv))
# 399th and 406th
```

**(h)**

```{r}
sum(Boston$rm > 7)

sum(Boston$rm > 8)

High.dwelling.Boston <- Boston[Boston$rm > 8, ]

summary(Boston)
summary(High.dwelling.Boston)
```

More average number of room per dwelling results in a lower crime rate

