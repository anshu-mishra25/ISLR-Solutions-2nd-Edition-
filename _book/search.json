[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISLR Solutions (2nd Edition)",
    "section": "",
    "text": "1 Introduction\nThis site is a personal documentation of the solutions of the book ‘An Introduction to Statistical Learning with R’ (2nd Edition)\nIf you’d like to replicate the code, make sure to have ‘ISLR2’ package installed and running in the environment.\ninstall.packages(\"ISLR2\")\nlibrary(ISLR2)\nYou can get the book from: https://www.statlearning.com/",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Ch02.html",
    "href": "Ch02.html",
    "title": "2 Statistical Learning",
    "section": "",
    "text": "Conceptual",
    "crumbs": [
      "2 Statistical Learning"
    ]
  },
  {
    "objectID": "Ch02.html#conceptual",
    "href": "Ch02.html#conceptual",
    "title": "2 Statistical Learning",
    "section": "",
    "text": "Question 1\n(a) Better: flexible method incorporates the data better when the sample size is large enough.\n(b) Worse: The data set leads to the ‘Curse of Dimensionality’, flexible model will tend to be more overfitting, meaning it will try to follow the error (noise) too closely.\n(c) Better: Flexible methods perform better on non-linear datasets as they have more degrees of freedom to approximate a non-linear.\n(d) Worse: A flexible model would likely overfit, due to more closely fitting the noise in the error terms than an inflexible method. In other words, the data points will be far from f (ideal function to describe the data) if the variance of the error terms is very high. This hints that that f is linear and so a simpler model would be better to be able to estimate f.\n\n\nQuestion 2\n(a) Regression Problem: Since it is a quantitative problem Inference: We are interested in the factors which affect CEO salary n = 500 p = 3 (Profit, number of employees, industry)\n(b) Classification Problem: Since it is binary (success or failure) Prediction: We are interested in the success or failure of the product n = 20 p = 13 (Price charged, marketing budget, competition price + 10 other)\n(c) Regression Problem Prediction: We are interested in the % change in the USD/Euro exchange rate n = 52 p = 3 (% change in the US market, % change in the British market, % change in the German market)\n\n\nQuestion 3\n(a) \n(b) Squared Bias: Decreases with more flexibility (generally more flexible methods results in less bias)\nVariance: Increases with more flexibility\nTraining Error: Continues to reduce as flexibility grows, meaning the model is too close to the training data.\nTest Error: Decreases initially, and reaches the optimal point where it gets flat, then it starts to increase again, meaning an overfitted data\nBayes (irreducible) Error: Flat/Fixed, since it can’t be reduced any more\n\n\nQuestion 4\n(a) Three real-life applications in which classification might be useful\n1. Email Spam Detection\n\nResponse: Spam(1) or Not Spam(0)\nPredictors: some special words, special links, other\nGoal: Prediction — we care more about classifying new emails correctly than about interpreting which word exactly causes spam.\n\n2. Loan Default Risk\n\nRespone: Default(1) or Won’t Default(0)\nPredictors: income, credit score, education, etc\nGoal: Prediction/Inference — banks want to predict default risk, but also understand which factors matter most\n\n3. Stock Price\n\nResponse: Stock increases(1) or Stock goes down(0)\nPredictors: Financial Statements, Ratio Analysis, Competitors price, etc\nGoal: Prediction — we are interested in making the future movements of the stock price.\n\n(b) Three real-life applications in which regression might be useful:\n1. House Price Estimation\n\nResponse: House price (in $).\nPredictors: square footage, number of bedrooms, location, age of house.\nGoal: Prediction — estimate the price of houses not yet sold.\n\n2. Salary Determination\n\nResponse: Employee salary (in $).\nPredictors: years of experience, education level, industry, job role.\nGoal: Inference — HR might want to know which factors (experience vs. education) most strongly drive salaries.\n\n3. Insurance Premium Calculation\n\nResponse: Annual premium amount.\nPredictors: age, medical history, smoking status, BMI.\nGoal: Both — insurers need prediction for pricing policies, but also inference to understand key risk drivers.\n\n(c) Three real-life applications in which cluster analysis might be useful:\n1. Customer Segmentation in Retail\n\nPredictors: purchase history, shopping frequency, spending amounts.\nGoal: Identify customer groups (e.g., bargain-hunters vs. high spenders) for targeted marketing.\n\n2. Genetic Research\n\nPredictors: gene expression levels.\nGoal: Cluster similar genes/patients to identify subtypes of diseases.\n\n3. Social Media Communities\n\nPredictors: interaction patterns, hashtags used, follower networks.\nGoal: Find natural clusters of users (e.g., sports fans, political groups, hobby communities).\n\n\n\nQuestion 5\nAdvantages of a More Flexible Approach\n\nBetter fit to complex patterns: Can capture nonlinear relationships and intricate interactions between variables.\nLower bias: Since fewer assumptions are made about the functional form, the model can adapt closely to the data.\nHigh predictive power: Often yields better test accuracy if enough data is available.\n\nDisadvantages of a More Flexible Approach\n\nRisk of overfitting: May capture noise in the training data instead of the true signal.\nHigher variance: Predictions can change drastically with new data.\nInterpretability: Flexible models are often “black boxes” (e.g., random forests, neural nets).\nComputational cost: More data and resources needed for training.\n\nAdvantages of a Less Flexible Approach\n\nSimplicity: Easy to implement, interpret, and explain to stakeholders.\nLow variance: Less sensitive to small changes in the dataset.\nEfficient: Works well with smaller datasets, requires less computation.\nInference: Easier to test hypotheses and understand relationships between predictors and response.\n\nDisadvantages of a Less Flexible Approach\n\nHigh bias: May oversimplify reality by assuming linearity or ignoring interactions.\nPoor fit for complex relationships: If the true relationship is nonlinear, performance suffers.\n\nWhen to prefer More Flexible Approaches\n\nWhen prediction accuracy is the main goal.\nWhen the relationship between predictors and response is complex and nonlinear.\nWhen you have large amounts of data to reduce variance and prevent overfitting.\n\nWhen to prefer Less Flexible Approaches\n\nWhen the primary goal is inference/interpretability (understanding which predictors matter).\nWhen you have limited data, flexible models overfit easily with small datasets.\nWhen stakeholders (like regulators, executives) need transparent models.\n\n\n\nQuestion 6\nParametric: Assume a functional form (e.g., linear regression), then estimate a few parameters.\n\nAdvantage: Works with small data, simple, interpretable, fast.\nDisadvantage: Risk of bias if model form is wrong, limited flexibility.\n\nNon-Parametric: No strict form, model is shaped by the data (e.g., KNN, trees).\n\nAdvantage: Very flexible, can capture complex patterns.\nDisadvantage: Needs lots of data, higher variance, less interpretable, slower.\n\n\n\nQuestion 7\n(a) Computing the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0\n\n(data.set &lt;- data.frame(\n  \"X1\" = c(0,2,0,0,-1,1),\n  \"X2\" = c(3,0,1,1,0,1),\n  \"X3\" = c(0,0,3,2,1,1),\n  \"Y\" = c(\"Red\",\"Red\",\"Red\",\"Green\",\"Green\",\"Red\")\n  ))   # A given data set\n\n  X1 X2 X3     Y\n1  0  3  0   Red\n2  2  0  0   Red\n3  0  1  3   Red\n4  0  1  2 Green\n5 -1  0  1 Green\n6  1  1  1   Red\n\n\n\neuclidian.distance &lt;- \n  function(X, pred.data){\n    # 'X' here represents a vector of points\n    distance = sqrt((X[1]-pred.data[1])^2 +\n                      (X[2]-pred.data[2])^2 +\n                      (X[3]-pred.data[3])^2) \n    return(distance)\n  }  # A function to compute euclidian distance\n\n\nX &lt;- data.set[,-4]    # Taking only the given X co-ordinates\nPrediction.coordinates &lt;- c(0,0,0) # Prediction co-ordinates\n\ndistance &lt;- numeric()\nfor(i in 1:nrow(X)){\n  distance[i] = as.matrix(euclidian.distance(X[i,], Prediction.coordinates))\n}\n\ndistance &lt;- as.matrix(distance)\n\ncat(\"\\n\",\n  \"dist(New,Obs1) =\",distance[1,1],\"\\n\",\n  \"dist(New,Obs2) =\",distance[2,1],\"\\n\",\n  \"dist(New,Obs3) =\",distance[3,1],\"\\n\",\n  \"dist(New,Obs4) =\",distance[4,1],\"\\n\",\n  \"dist(New,Obs5) =\",distance[5,1],\"\\n\",\n  \"dist(New,Obs6) =\",distance[6,1],\"\\n\"\n  )\n\n\n dist(New,Obs1) = 3 \n dist(New,Obs2) = 2 \n dist(New,Obs3) = 3.162278 \n dist(New,Obs4) = 2.236068 \n dist(New,Obs5) = 1.414214 \n dist(New,Obs6) = 1.732051 \n\n\n(b)\n\n(Y &lt;- data.set[,4]) # All possible given ouputs\n\n[1] \"Red\"   \"Red\"   \"Red\"   \"Green\" \"Green\" \"Red\"  \n\nKNN &lt;- \n  function(K){\n    Y[which.min(abs(distance[,1]-K))]\n  }\n\ncat(\"K-Nearest Neighbour for K = 1 predicts it to be\",KNN(1))\n\nK-Nearest Neighbour for K = 1 predicts it to be Green\n\n\n(c)\n\ncat(\"K-Nearest Neighbour for K = 3 predicts it to be\",KNN(3))\n\nK-Nearest Neighbour for K = 3 predicts it to be Red",
    "crumbs": [
      "2 Statistical Learning"
    ]
  },
  {
    "objectID": "Ch02.html#applied",
    "href": "Ch02.html#applied",
    "title": "2 Statistical Learning",
    "section": "APPLIED",
    "text": "APPLIED\n\nQuestion 8\nRunning the ‘ISLR2’ package\n\n# install.packages(\"ISLR2\")\nlibrary(ISLR2)\n\n(a)\n\ncollege &lt;- College # naming as per question\n\n(b)\n\n# View(college) #\nhead(college)\n\n                             Private Apps Accept Enroll Top10perc Top25perc\nAbilene Christian University     Yes 1660   1232    721        23        52\nAdelphi University               Yes 2186   1924    512        16        29\nAdrian College                   Yes 1428   1097    336        22        50\nAgnes Scott College              Yes  417    349    137        60        89\nAlaska Pacific University        Yes  193    146     55        16        44\nAlbertson College                Yes  587    479    158        38        62\n                             F.Undergrad P.Undergrad Outstate Room.Board Books\nAbilene Christian University        2885         537     7440       3300   450\nAdelphi University                  2683        1227    12280       6450   750\nAdrian College                      1036          99    11250       3750   400\nAgnes Scott College                  510          63    12960       5450   450\nAlaska Pacific University            249         869     7560       4120   800\nAlbertson College                    678          41    13500       3335   500\n                             Personal PhD Terminal S.F.Ratio perc.alumni Expend\nAbilene Christian University     2200  70       78      18.1          12   7041\nAdelphi University               1500  29       30      12.2          16  10527\nAdrian College                   1165  53       66      12.9          30   8735\nAgnes Scott College               875  92       97       7.7          37  19016\nAlaska Pacific University        1500  76       72      11.9           2  10922\nAlbertson College                 675  67       73       9.4          11   9727\n                             Grad.Rate\nAbilene Christian University        60\nAdelphi University                  56\nAdrian College                      54\nAgnes Scott College                 59\nAlaska Pacific University           15\nAlbertson College                   55\n\n\n(c) \n\n# i.\nsummary(college)\n\n Private        Apps           Accept          Enroll       Top10perc    \n No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00  \n Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00  \n           Median : 1558   Median : 1110   Median : 434   Median :23.00  \n           Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56  \n           3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00  \n           Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00  \n   Top25perc      F.Undergrad     P.Undergrad         Outstate    \n Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340  \n 1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320  \n Median : 54.0   Median : 1707   Median :  353.0   Median : 9990  \n Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441  \n 3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925  \n Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700  \n   Room.Board       Books           Personal         PhD        \n Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00  \n 1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00  \n Median :4200   Median : 500.0   Median :1200   Median : 75.00  \n Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66  \n 3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00  \n Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00  \n    Terminal       S.F.Ratio      perc.alumni        Expend     \n Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186  \n 1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751  \n Median : 82.0   Median :13.60   Median :21.00   Median : 8377  \n Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660  \n 3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830  \n Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233  \n   Grad.Rate     \n Min.   : 10.00  \n 1st Qu.: 53.00  \n Median : 65.00  \n Mean   : 65.46  \n 3rd Qu.: 78.00  \n Max.   :118.00  \n\n\n\n# ii.\npairs(college[,1:10], cex = 0.1)\n\n\n\n\n\n\n\n\n\n# iii.\nboxplot(Outstate ~ Private, data = college)\n\n\n\n\n\n\n\n\n\n# iv.\ncollege$Elite &lt;- as.factor(ifelse(college$Top10perc &gt; 50, \"Yes\", \"No\"))\n\nsummary(college$Elite)\n\n No Yes \n699  78 \n\n\n\n# v.\npar(mfrow = c(2,2))\n\n# FOR COLLEGE APPLICATIONS RECEIVED\nfor(n in c(5,10,15,20)){\n  hist(college$Apps, \n       xlab = \"Number of Applications Received\",\n       main = \"Histogram of College Application\",\n       breaks = n,\n       xlim = c(0,20000))\n}\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2,2))\n\n# FOR COLLEGE APPLICATIONS ACCEPTED\nfor(n in c(5,10,15,20)){\n  hist(college$Accept, \n       xlab = \"Number of Applications Accepted\",\n       main = \"Histogram of College Application\",\n       breaks = n,\n       xlim = c(0,20000))\n}\n\n\n\n\n\n\n\n\n\npar(mfrow = c(1,1))\n\n\n# vi.\n# Making a linear model with all variables\nModel1 &lt;- lm(Accept~.,data=College)\nsummary(Model1)\n\n\nCall:\nlm(formula = Accept ~ ., data = College)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3628.2  -186.3     5.1   197.0  3476.0 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.896e+02  2.101e+02  -1.378  0.16850    \nPrivateYes   1.654e+02  7.128e+01   2.320  0.02058 *  \nApps         4.201e-01  1.079e-02  38.924  &lt; 2e-16 ***\nEnroll       1.162e+00  8.749e-02  13.276  &lt; 2e-16 ***\nTop10perc   -2.765e+01  2.847e+00  -9.713  &lt; 2e-16 ***\nTop25perc    9.246e+00  2.296e+00   4.026 6.24e-05 ***\nF.Undergrad -1.870e-02  1.686e-02  -1.109  0.26773    \nP.Undergrad -3.368e-02  1.652e-02  -2.039  0.04183 *  \nOutstate     6.090e-02  9.690e-03   6.286 5.51e-10 ***\nRoom.Board  -1.196e-02  2.501e-02  -0.478  0.63272    \nBooks        1.580e-02  1.227e-01   0.129  0.89756    \nPersonal    -4.552e-02  3.243e-02  -1.404  0.16080    \nPhD          4.680e+00  2.387e+00   1.961  0.05025 .  \nTerminal     6.402e-01  2.623e+00   0.244  0.80724    \nS.F.Ratio   -4.666e+00  6.699e+00  -0.697  0.48627    \nperc.alumni -5.529e+00  2.102e+00  -2.630  0.00871 ** \nExpend      -2.952e-02  6.432e-03  -4.590 5.19e-06 ***\nGrad.Rate   -1.231e+00  1.526e+00  -0.807  0.42000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 536 on 759 degrees of freedom\nMultiple R-squared:  0.9532,    Adjusted R-squared:  0.9522 \nF-statistic: 909.9 on 17 and 759 DF,  p-value: &lt; 2.2e-16\n\n# Using this model to determine all the significant parameters which comes out \n# to be:\n# -&gt; Private\n# -&gt; Apps\n# -&gt; Enroll\n# -&gt; Top10perc\n# -&gt; Top25perc\n# -&gt; P.Undergrad\n# -&gt; Outstate\n# -&gt; perc.alumni\n# -&gt; Expend\n# Making another model with these parameters only:\n\nModel2 &lt;- lm(Accept ~ Private + Apps + Enroll + Top10perc + Top25perc +\n               P.Undergrad + Outstate + perc.alumni + Expend,\n             data=College)\nsummary(Model2)\n\n\nCall:\nlm(formula = Accept ~ Private + Apps + Enroll + Top10perc + Top25perc + \n    P.Undergrad + Outstate + perc.alumni + Expend, data = College)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3728.9  -187.9    -2.3   212.3  3550.0 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.761e+02  9.134e+01  -3.023  0.00258 ** \nPrivateYes   1.120e+02  6.557e+01   1.709  0.08788 .  \nApps         4.168e-01  1.047e-02  39.810  &lt; 2e-16 ***\nEnroll       1.085e+00  4.534e-02  23.933  &lt; 2e-16 ***\nTop10perc   -2.740e+01  2.824e+00  -9.700  &lt; 2e-16 ***\nTop25perc    1.005e+01  2.242e+00   4.483 8.49e-06 ***\nP.Undergrad -3.612e-02  1.552e-02  -2.327  0.02020 *  \nOutstate     6.801e-02  8.312e-03   8.182 1.16e-15 ***\nperc.alumni -4.846e+00  2.015e+00  -2.405  0.01639 *  \nExpend      -2.678e-02  5.882e-03  -4.552 6.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 538 on 767 degrees of freedom\nMultiple R-squared:  0.9524,    Adjusted R-squared:  0.9518 \nF-statistic:  1705 on 9 and 767 DF,  p-value: &lt; 2.2e-16\n\n# Adjusted R-squared dropped but only by 0.0004, that doesn't make Model1 any\n# better than Model2 with so much less parameters.\n\nAIC(Model1)\n\n[1] 11990.35\n\nAIC(Model2)\n\n[1] 11988.27\n\n# AIC dropped\n\n# Another Model with even less parameters\nModel3 &lt;- update(Model2, .~. -Private - P.Undergrad - perc.alumni)\nsummary(Model3)\n\n\nCall:\nlm(formula = Accept ~ Apps + Enroll + Top10perc + Top25perc + \n    Outstate + Expend, data = College)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3758.7  -180.9    -3.7   206.5  3618.8 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.766e+02  8.696e+01  -3.180  0.00153 ** \nApps         4.186e-01  1.039e-02  40.308  &lt; 2e-16 ***\nEnroll       1.034e+00  4.266e-02  24.240  &lt; 2e-16 ***\nTop10perc   -2.695e+01  2.816e+00  -9.569  &lt; 2e-16 ***\nTop25perc    9.260e+00  2.245e+00   4.125 4.12e-05 ***\nOutstate     7.040e-02  7.129e-03   9.875  &lt; 2e-16 ***\nExpend      -2.865e-02  5.896e-03  -4.859 1.43e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 541.7 on 770 degrees of freedom\nMultiple R-squared:  0.9515,    Adjusted R-squared:  0.9512 \nF-statistic:  2520 on 6 and 770 DF,  p-value: &lt; 2.2e-16\n\nAIC(Model3)\n\n[1] 11995.87\n\n# AIC increased by ~2 points\n\nanova(Model2, Model3)\n\nAnalysis of Variance Table\n\nModel 1: Accept ~ Private + Apps + Enroll + Top10perc + Top25perc + P.Undergrad + \n    Outstate + perc.alumni + Expend\nModel 2: Accept ~ Apps + Enroll + Top10perc + Top25perc + Outstate + Expend\n  Res.Df       RSS Df Sum of Sq     F   Pr(&gt;F)   \n1    767 221996912                               \n2    770 225916474 -3  -3919562 4.514 0.003789 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Another Model\nModel4 &lt;- update(Model2, .~. -Private)\nsummary(Model4)\n\n\nCall:\nlm(formula = Accept ~ Apps + Enroll + Top10perc + Top25perc + \n    P.Undergrad + Outstate + perc.alumni + Expend, data = College)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3714.3  -189.2     0.3   213.5  3601.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.315e+02  8.762e+01  -2.642  0.00842 ** \nApps         4.152e-01  1.044e-02  39.763  &lt; 2e-16 ***\nEnroll       1.068e+00  4.434e-02  24.098  &lt; 2e-16 ***\nTop10perc   -2.725e+01  2.827e+00  -9.642  &lt; 2e-16 ***\nTop25perc    9.827e+00  2.241e+00   4.385 1.32e-05 ***\nP.Undergrad -3.952e-02  1.541e-02  -2.565  0.01050 *  \nOutstate     7.404e-02  7.534e-03   9.828  &lt; 2e-16 ***\nperc.alumni -4.540e+00  2.009e+00  -2.259  0.02415 *  \nExpend      -2.720e-02  5.884e-03  -4.623 4.43e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 538.7 on 768 degrees of freedom\nMultiple R-squared:  0.9522,    Adjusted R-squared:  0.9517 \nF-statistic:  1912 on 8 and 768 DF,  p-value: &lt; 2.2e-16\n\nAIC(Model2); AIC(Model4)\n\n[1] 11988.27\n\n\n[1] 11989.23\n\nanova(Model2,Model4)\n\nAnalysis of Variance Table\n\nModel 1: Accept ~ Private + Apps + Enroll + Top10perc + Top25perc + P.Undergrad + \n    Outstate + perc.alumni + Expend\nModel 2: Accept ~ Apps + Enroll + Top10perc + Top25perc + P.Undergrad + \n    Outstate + perc.alumni + Expend\n  Res.Df       RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    767 221996912                              \n2    768 222842138 -1   -845226 2.9203 0.08788 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOverall Model 2 is better, but if I wanted to use as less co-variates as possible in order to explain most of the response variable, I can use Model 4 as well.\n\n\nQuestion 9\n(a)\n\nhead(Auto)\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n5  17         8          302        140   3449         10.5   70      1\n6  15         8          429        198   4341         10.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n5               ford torino\n6          ford galaxie 500\n\n\nQuantitative: mpg, cylinders, displacement, horsepower, weight, acceleration, year Qualitative: origin, name\n(b)\n\napply(Auto[,1:7], 2, range)\n\n      mpg cylinders displacement horsepower weight acceleration year\n[1,]  9.0         3           68         46   1613          8.0   70\n[2,] 46.6         8          455        230   5140         24.8   82\n\n\n(c)\n\napply(Auto[,1:7], 2, mean)\n\n         mpg    cylinders displacement   horsepower       weight acceleration \n   23.445918     5.471939   194.411990   104.469388  2977.584184    15.541327 \n        year \n   75.979592 \n\napply(Auto[,1:7], 2, sd)\n\n         mpg    cylinders displacement   horsepower       weight acceleration \n    7.805007     1.705783   104.644004    38.491160   849.402560     2.758864 \n        year \n    3.683737 \n\n\n(d)\n\nAuto.reduced &lt;- Auto[-c(10:85), ]\n\napply(Auto.reduced[,1:7], 2, range)\n\n      mpg cylinders displacement horsepower weight acceleration year\n[1,] 11.0         3           68         46   1649          8.5   70\n[2,] 46.6         8          455        230   4997         24.8   82\n\napply(Auto.reduced[,1:7], 2, mean)\n\n         mpg    cylinders displacement   horsepower       weight acceleration \n   24.404430     5.373418   187.240506   100.721519  2935.971519    15.726899 \n        year \n   77.145570 \n\napply(Auto.reduced[,1:7], 2, sd)\n\n         mpg    cylinders displacement   horsepower       weight acceleration \n    7.867283     1.654179    99.678367    35.708853   811.300208     2.693721 \n        year \n    3.106217 \n\n\n(e)\n\npairs(Auto[,1:7], cex = 0.5, pch = 16)\n\n\n\n\n\n\n\ncor(Auto[,1:7])\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\n             acceleration       year\nmpg             0.4233285  0.5805410\ncylinders      -0.5046834 -0.3456474\ndisplacement   -0.5438005 -0.3698552\nhorsepower     -0.6891955 -0.4163615\nweight         -0.4168392 -0.3091199\nacceleration    1.0000000  0.2903161\nyear            0.2903161  1.0000000\n\n\nThe covariates with high positive correlation are (more than 0.7): cylinders vs displacement cylinders vs horsepower cylinders vs weight displacement vs horsepower displacement vs weight horsepower vs weight\nThe covariates with high negative correlation are (less than -0.7): mpg vs cylinders mpg vs displacement mpg vs horsepower mpg vs weight\n(f)\nYes, all the other variables except acceleration (maybe even year) are highly correlated and can be used to predict mpg.\n\n\nQuestion 10\n(a)\n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\n\n\n\ncat(\"\\n\",\n    \"Number of rows =\",nrow(Boston),\"\\n\",\n    \"Number of columns =\",ncol(Boston))\n\n\n Number of rows = 506 \n Number of columns = 13\n\n\nEach 13 column suggests a variable affecting the price of the particular suburb. There are 506 suburbs each with 13 explanatory variables.\n(b)\n\npairs(Boston, cex = 0.5)\n\n\n\n\n\n\n\ncor(Boston)\n\n               crim          zn       indus         chas         nox\ncrim     1.00000000 -0.20046922  0.40658341 -0.055891582  0.42097171\nzn      -0.20046922  1.00000000 -0.53382819 -0.042696719 -0.51660371\nindus    0.40658341 -0.53382819  1.00000000  0.062938027  0.76365145\nchas    -0.05589158 -0.04269672  0.06293803  1.000000000  0.09120281\nnox      0.42097171 -0.51660371  0.76365145  0.091202807  1.00000000\nrm      -0.21924670  0.31199059 -0.39167585  0.091251225 -0.30218819\nage      0.35273425 -0.56953734  0.64477851  0.086517774  0.73147010\ndis     -0.37967009  0.66440822 -0.70802699 -0.099175780 -0.76923011\nrad      0.62550515 -0.31194783  0.59512927 -0.007368241  0.61144056\ntax      0.58276431 -0.31456332  0.72076018 -0.035586518  0.66802320\nptratio  0.28994558 -0.39167855  0.38324756 -0.121515174  0.18893268\nlstat    0.45562148 -0.41299457  0.60379972 -0.053929298  0.59087892\nmedv    -0.38830461  0.36044534 -0.48372516  0.175260177 -0.42732077\n                 rm         age         dis          rad         tax    ptratio\ncrim    -0.21924670  0.35273425 -0.37967009  0.625505145  0.58276431  0.2899456\nzn       0.31199059 -0.56953734  0.66440822 -0.311947826 -0.31456332 -0.3916785\nindus   -0.39167585  0.64477851 -0.70802699  0.595129275  0.72076018  0.3832476\nchas     0.09125123  0.08651777 -0.09917578 -0.007368241 -0.03558652 -0.1215152\nnox     -0.30218819  0.73147010 -0.76923011  0.611440563  0.66802320  0.1889327\nrm       1.00000000 -0.24026493  0.20524621 -0.209846668 -0.29204783 -0.3555015\nage     -0.24026493  1.00000000 -0.74788054  0.456022452  0.50645559  0.2615150\ndis      0.20524621 -0.74788054  1.00000000 -0.494587930 -0.53443158 -0.2324705\nrad     -0.20984667  0.45602245 -0.49458793  1.000000000  0.91022819  0.4647412\ntax     -0.29204783  0.50645559 -0.53443158  0.910228189  1.00000000  0.4608530\nptratio -0.35550149  0.26151501 -0.23247054  0.464741179  0.46085304  1.0000000\nlstat   -0.61380827  0.60233853 -0.49699583  0.488676335  0.54399341  0.3740443\nmedv     0.69535995 -0.37695457  0.24992873 -0.381626231 -0.46853593 -0.5077867\n             lstat       medv\ncrim     0.4556215 -0.3883046\nzn      -0.4129946  0.3604453\nindus    0.6037997 -0.4837252\nchas    -0.0539293  0.1752602\nnox      0.5908789 -0.4273208\nrm      -0.6138083  0.6953599\nage      0.6023385 -0.3769546\ndis     -0.4969958  0.2499287\nrad      0.4886763 -0.3816262\ntax      0.5439934 -0.4685359\nptratio  0.3740443 -0.5077867\nlstat    1.0000000 -0.7376627\nmedv    -0.7376627  1.0000000\n\n\n\nmedv parameter has good positive correlation with rm and high negative correlation with lstat\nindus parameter has high positive correlation with nox and tax, and high negative correlation with dis\nand many more…\n\n(c)\ncrm (per capita crime rate by town) has the highest correlation with rad (index of accesssibility to radial highways) which is positive.\n(d)\n\n# High Crime Rates\nHigh.Crime.Rate &lt;- \n  Boston$crim[Boston$crim &gt; mean(Boston$crim) + 2*sd(Boston$crim)]\ncat(\"There are\",length(High.Crime.Rate),\"suburbs which have high crime rate\")\n\nThere are 16 suburbs which have high crime rate\n\nhist(High.Crime.Rate,\n     xlab = \"Crime Rate\",\n     main = \"Histogram of Suburbs with High Crime Rates\")\n\n\n\n\n\n\n\nrange(High.Crime.Rate)\n\n[1] 22.0511 88.9762\n\n\n\n# High Tax Rates\nHigh.Tax.Rate &lt;- \n  Boston$tax[Boston$tax &gt; mean(Boston$tax) + 2*sd(Boston$tax)]\n\ncat(\"There are\",length(High.Tax.Rate), \"suburbs which have high tax rate\")\n\nThere are 0 suburbs which have high tax rate\n\n\n\n# High Pupil-teacher ratios\nHigh.Pupil.teacher.ratios &lt;- \n  Boston$ptratio[Boston$ptratio &gt; mean(Boston$ptratio) + 2*sd(Boston$ptratio)]\n\ncat(\"There are\",length(High.Pupil.teacher.ratios),\n    \"suburbs which have high Pupil-teacher ratio\")\n\nThere are 0 suburbs which have high Pupil-teacher ratio\n\n\n(e)\ncat(“There are”,sum(Boston$chas),“suburbs that bound the Charles river.”)\n(f)\ncat(“Median pupil-teacher ratio in town is”,median(Boston$ptratio))\n(g)\n\nwhich(Boston$medv == min(Boston$medv))\n\n[1] 399 406\n\n# 399th and 406th\n\n(h)\n\nsum(Boston$rm &gt; 7)\n\n[1] 64\n\nsum(Boston$rm &gt; 8)\n\n[1] 13\n\nHigh.dwelling.Boston &lt;- Boston[Boston$rm &gt; 8, ]\n\nsummary(Boston)\n\n      crim                zn             indus            chas        \n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n 1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  \n Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  \n 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n      nox               rm             age              dis        \n Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  \n Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  \n Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  \n 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  \n Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n      rad              tax           ptratio          lstat      \n Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   : 1.73  \n 1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.: 6.95  \n Median : 5.000   Median :330.0   Median :19.05   Median :11.36  \n Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :12.65  \n 3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:16.95  \n Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :37.97  \n      medv      \n Min.   : 5.00  \n 1st Qu.:17.02  \n Median :21.20  \n Mean   :22.53  \n 3rd Qu.:25.00  \n Max.   :50.00  \n\nsummary(High.dwelling.Boston)\n\n      crim               zn            indus             chas       \n Min.   :0.02009   Min.   : 0.00   Min.   : 2.680   Min.   :0.0000  \n 1st Qu.:0.33147   1st Qu.: 0.00   1st Qu.: 3.970   1st Qu.:0.0000  \n Median :0.52014   Median : 0.00   Median : 6.200   Median :0.0000  \n Mean   :0.71880   Mean   :13.62   Mean   : 7.078   Mean   :0.1538  \n 3rd Qu.:0.57834   3rd Qu.:20.00   3rd Qu.: 6.200   3rd Qu.:0.0000  \n Max.   :3.47428   Max.   :95.00   Max.   :19.580   Max.   :1.0000  \n      nox               rm             age             dis       \n Min.   :0.4161   Min.   :8.034   Min.   : 8.40   Min.   :1.801  \n 1st Qu.:0.5040   1st Qu.:8.247   1st Qu.:70.40   1st Qu.:2.288  \n Median :0.5070   Median :8.297   Median :78.30   Median :2.894  \n Mean   :0.5392   Mean   :8.349   Mean   :71.54   Mean   :3.430  \n 3rd Qu.:0.6050   3rd Qu.:8.398   3rd Qu.:86.50   3rd Qu.:3.652  \n Max.   :0.7180   Max.   :8.780   Max.   :93.90   Max.   :8.907  \n      rad              tax           ptratio          lstat           medv     \n Min.   : 2.000   Min.   :224.0   Min.   :13.00   Min.   :2.47   Min.   :21.9  \n 1st Qu.: 5.000   1st Qu.:264.0   1st Qu.:14.70   1st Qu.:3.32   1st Qu.:41.7  \n Median : 7.000   Median :307.0   Median :17.40   Median :4.14   Median :48.3  \n Mean   : 7.462   Mean   :325.1   Mean   :16.36   Mean   :4.31   Mean   :44.2  \n 3rd Qu.: 8.000   3rd Qu.:307.0   3rd Qu.:17.40   3rd Qu.:5.12   3rd Qu.:50.0  \n Max.   :24.000   Max.   :666.0   Max.   :20.20   Max.   :7.44   Max.   :50.0  \n\n\nMore average number of room per dwelling results in a lower crime rate",
    "crumbs": [
      "2 Statistical Learning"
    ]
  }
]